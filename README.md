# stores_project
Создание отчета по работе сети магазинов

Исходные данные: 
Справочники - файлы CSV: 'coupons.csv' - купоны, 'promos.csv' - данные о промоакциях, 'promo_types.csv' - типы промоакций, 'stores.scv' - список магазинов сети
Таблицы фактов - таблицы базы Postgres: 'bills_head' - чеки, 'bills_item' - данные по чекам, 'traffic' - трафик

Шаги:
1. Функция 'f_full_load_ip' (на plpgsql, тип загрузки full) для загрузки данных из файлов CSV через протокол gpfdist в Greenplum
2. Функции 'f_bills' и 'f_traffic' (на plpgsql, тип загрузки Delta partition) для загрузки данных из таблиц базы Postgres через протокол pxf в Greenplum
4. Функция 'f_mart' (на plpgsql) для создания витрины данных по работе сети магазинов (вычисление параметров: Оборот,	Скидки по купонам,	Оборот с учетом скидки,	Кол-во проданных товаров,	Количество чеков,	Трафик,	Кол-во товаров по акции,	Доля товаров со скидкой,	Среднее количество товаров в чеке, 	Коэффициент конверсии магазина, %, 	Средний чек, руб, Средняя выручка на одного посетителя, руб) в Greenplum
5. Автоматизация процесса загрузки данных и сборки витрины с помощью Apache Airflow (файл load_mart_dag.py):
   DAG 'stores_dag' (на python) удалённо запускает в Greenplum пользовательские функции 'f_full_load_ip', 'f_bills', 'f_traffic', 'f_mart'
6. Загрузка витрины данных в Clickhouse (файл CH_mart.sql)
7. Формирование отчета в Apache Superset (возможность формирования отчета помесячно/подневно)


